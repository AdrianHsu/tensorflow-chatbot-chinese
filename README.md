#  tensorflow-chatbot-chinese

> ğŸƒ [Chinese chatbot] tensorflow implementation of seq2seq model with bahdanau attention and Word2Vec pretrained embedding

<p align=center><a target="_blank" href="https://opensource.org/licenses/MIT" title="License: MIT"><img src="https://img.shields.io/badge/License-MIT-blue.svg"></a><a target="_blank" href="http://makeapullrequest.com" title="PRs Welcome"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg"></a></p>  



This repo is based on [this article](https://blog.csdn.net/liuchonge/article/details/79021938), written by [liuchongee](https://blog.csdn.net/liuchonge).
![](img/result.gif)





## How-to

#### [option 1] re-train the model

```shell
$ git clone https://github.com/AdrianHsu/tensorflow-chatbot-chinese.git
# put your own training/eval data in the correct path, as shown above
$ ./run.sh
```

#### [option 2] testing the model with pretrained saver file

1. You should download the pretrained model [here](/) and then put it into `save/`directory.
2. make sure your input is already put in the correct path, and also it is pre-processed by **text segmentation** APIs, for example, *jieba*.

```
$ ./hw2_seq2seq.sh
```



## How-to (web)

You have to download the frozen model first.

```
$ ./download_model.sh
```

And then just directly run the server file.

```
$ python3 server.py
```

You'll need `aiohttp`, `python-socketio` to run this. The result will be shown on `https://localhost:8080`.  
WARNING: do not install `socketio`, this is not the same as `python-socketio`!!

## Hyperparameters

```Python
MAX_SENTENCE_LENGTH = 15 # abandon those sentences with > 15 words
special_tokens = {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2, '<UNK>': 3}
emb_size       =     300

# Model parameters

num_layers = 2
rnn_size   = 2048 # GRU cell
keep_prob  = 1.0
vocab_num  = 10561
min_cnt    = 100 # min counts, we only preserver those words appears >= 100 times
num_epochs = 50
batch_size = 250

lr         = 0.5
lr = tf.train.exponential_decay(...
       decay_steps=3250, decay_rate=0.95, staircase=True)
```

- optimizer: `GradientDescentOptimizer()`
- schedule sampling: `inverse sigmoid(0.88 to 0.5)`
- attention method: [Bahdanau Attention](https://arxiv.org/abs/1409.0473)
- random seed are all fixed to `0`.





## Model Structure

![](./img/model.png)

- Attention Method: Bahdanau Attention
- RNN Cell : GRU





## Directory Tree (source code)

```Shell
$ tree -L 1
.
â”œâ”€â”€ clean.sh # clean up the `logs/` and `save/`
â”œâ”€â”€ embeddings.npy # [vocab_num, emb_size] matrix, not used in the final version
â”œâ”€â”€ handler.py # data handler
â”œâ”€â”€ hw2_seq2seq.sh # script for testing data
â”œâ”€â”€ logs # logs for tensorboard
â”œâ”€â”€ model_seq2seq.py # training, validation, inference session & model buliding
â”œâ”€â”€ output.txt # output default path for testing
â”œâ”€â”€ perplexity.sh # not used
â”œâ”€â”€ README.md
â”œâ”€â”€ run.sh # re-train the model
â”œâ”€â”€ save # model saver checkpoint
â”œâ”€â”€ test_input.txt # input default path for training
â”œâ”€â”€ util.py # print function, inverse sigmoid sampling function
â”œâ”€â”€ idx2word.pkl
â”œâ”€â”€ word2idx.pkl
â””â”€â”€ word2vec.model # generated by gensim Word2Vec model 
```



## Directory Tree (data)

The data for training/validation is **only** a txt file named **conversation.txt**, and I put it in:

```
/home/data/mlds_hw2_2_data/conversation.txt
```

You could modify the path through passing an argument `â€”data_dir`.

Also, if you want to use your own data, you should modify the `model_seq2seq.py` , which is originally set to: 

```Python
filename = '/conversation.txt' # text segmentation is already finished
total_line_num = 3599478
train_line_num = 3587000
eval_line_num  =   12478 

emb_size       =     300 # embedding size
PKL_EXIST      =    True # if you want to re-train the GenSim model, you should set it False
```



## Data Format

##### conversation.txt

```Shell
é€™ ä¸æ˜¯ ä¸€æ™‚ èµ·æ„ çš„ è¡Œåˆº
è€Œæ˜¯ æœ‰ æ”¿æ²» å‹•æ©Ÿ
ä¸Šæ ¡ , é€™ç¨® äº‹
+++$+++
ä»– çš„ å£è¢‹ æ˜¯ ç©º çš„
æ²’æœ‰ çš®å¤¾ , ä¹Ÿ æ²’æœ‰ èº«åˆ†è­‰
æ‰‹éŒ¶ åœ åœ¨ 4 é» 15 åˆ†
å¤§æ¦‚ æ˜¯ å¢œæ©Ÿ çš„ æ™‚åˆ»
ä»– çš„ é™è½å‚˜ è¢« æ¨¹æ çºä½ äº†
```

1. Every dialogs are split by `+++$+++`
2. the prior sentence will be input, and the sentence itself will be ground truth (pair format)
3. for example (this is for training/validation):

```
[[['é€™', 'ä¸æ˜¯', 'ä¸€æ™‚', 'èµ·æ„', 'çš„', 'è¡Œåˆº'], ['è€Œæ˜¯', 'æœ‰', 'æ”¿æ²»', 'å‹•æ©Ÿ']],
[['è€Œæ˜¯', 'æœ‰', 'æ”¿æ²»', 'å‹•æ©Ÿ'], [â€˜ä¸Šæ ¡â€™, 'ï¼Œ', 'é€™ç¨®', 'äº‹']], ...]
```



## Number of Data

```Shell
# train
original line num: 3587000
used data num:     1642549
# validation
original line num:   12478
used data num:        4762
```



### Data preprocessing

1. `min_count` : 100
2. `MAX_SENTENCE_LENGTH`: 15
3. `MIN_SENTENCE_LENGTH`: 2, therefore, those sentence with only 1 word will be abandoned
4. `unk_num / float(len(sent)) > 0.1`: for example, if there are 10 words in this sentence, with only 1 <UNK>, then it will be preserved



## Pretrained Word2Vec using Gensim

```Python
raw_line.append(['<PAD>'] * 3999999) # to ensure <PAD> will be in index 0 (after sort)
raw_line.append(['<BOS>'] * 100)
raw_line.append(['<EOS>'] * 50)
raw_line.append(['<UNK>'] * 2999999)  # to ensure <UNK> will be in index 3 (after sort)

...
(in for loop)
line = text_to_word_sequence(line, lower=True, split=" ") # Keras API
line.insert(0, '<BOS>')
line.append('<EOS>')
raw_line.append(line)
...
self.model = Word2Vec(raw_line, size=emb_size, 
                      sorted_vocab=1, min_count=min_count, workers=4)

```



## Experimental Results

It takes about **30 hours** (with one 1080Ti ) to make the training loss decrease from **5.8** to **2.5**, (the generated sentence looks quite okay after training loss <= 2.5).

![](img/result.png)

### Correct description

```
Questions: ['å¥½', 'äº†', 'å¤§å®¶', 'åˆ°æ­¤çˆ²æ­¢', 'äº†', 'ã€‚']
Answers : ['è«‹', 'è®“', 'ç•¶äº‹äºº', 'é›¢é–‹', 'è®“', 'ä»–', 'èµ°', 'å§', 'ã€‚', '<EOS>'] 
Predict : ['è«‹', 'è®“', 'ç•¶äº‹äºº', 'é›¢é–‹', 'æˆ‘', 'å°±', 'èµ°', 'å§', 'ã€‚', '<EOS>']

Questions: ['é€™ä¸‹', 'è®“', 'æˆ‘', 'çœ‹çœ‹', 'å•Š', 'ã€‚']
Answers : ['å†çµ¦', 'æˆ‘', 'ä¸€åˆ†é˜', 'ä¸è¡Œ', 'å—', 'ã€‚', '<EOS>'] 
Predict : ['å†çµ¦', 'æˆ‘', 'ä¸€åˆ†é˜', 'ä¸è¡Œ', 'ã€‚', '<EOS>'] 

Questions: ['é«˜', 'è­šå¸‚', 'ç›¡', 'åœ¨', 'ä½ ', 'æŒæ¡', 'ä¹‹ä¸­', 'äº†', 'ã€‚']
Answers : ['é€™æ¨£', 'åš', 'æ˜¯', 'ä¸', 'å°', 'çš„', 'ã€‚', '<EOS>']
Predict : ['é€™æ¨£', 'åš', 'æ˜¯', 'ä¸', 'å°', 'çš„', 'ã€‚', '<EOS>']
```



### Irrelevant but reasonable descriptions

```
Questions: ['åˆ¥', 'æ¿€å‹•', 'åˆ¥', 'æ¿€å‹•', 'ã€‚']
Answers : ['æŠŠ', 'æ­¦å™¨', 'æ”¾ä¸‹', 'ã€‚', '<EOS>'] (len: 16, eos: 5)
Predict : ['æˆ‘å€‘', 'æ²’äº‹', 'å§', 'ã€‚', '<EOS>']
 
Questions: ['æœ€ç³Ÿ', 'çš„', 'å°±æ˜¯', 'è®“', 'ä»–', 'çŸ¥é“', 'ã€‚']
Answers : ['æˆ‘å€‘', 'å®³æ€•', 'ã€‚', '<EOS>']
Predict : ['ä»–', 'æ˜¯', 'æˆ‘', 'çš„', 'å…’å­', 'çš„', 'ã€‚', '<EOS>']

Questions: ['èµ·ä¾†', 'ä½ ', 'æ²’äº‹', 'å§', 'ã€‚']
Answers : ['ä¸å¥½æ„æ€', 'å§å§', 'å‰›æ‰', 'çœŸçš„', 'æ²’', 'çœ‹åˆ°', 'ä½ ', 'ã€‚', '<EOS>'] 
Predict : ['æ²’äº‹', 'ã€‚', '<EOS>']

Questions: ['æˆ‘æ„›ä½ ', 'ã€‚']
Answers : ['ä¸Šæ˜ŸæœŸ', 'æˆ‘', 'åœ¨', 'æ•™å ‚', 'å•', 'ä¸Šå¸', 'ã€‚', '<EOS>'] 
Predict : ['æˆ‘', 'ä¹Ÿ', 'æ„›', 'ä½ ', 'ã€‚', '<EOS>']
```


## Training / Evaluation Loss

It takes about **30 hours** (with one 1080Ti ) to make the training loss decrease from **5.8** to **2.5**, (the generated sentence looks quite okay after training loss <= 2.5).

![](img/loss.png)

